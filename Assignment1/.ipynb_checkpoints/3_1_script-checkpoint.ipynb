{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import open3d\n",
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "from open3d import geometry\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def show_fitting_result(pcds_list):\n",
    "    point_clouds_object_list = []\n",
    "    pc = open3d.PointCloud()\n",
    "    for i, pcd in enumerate(pcds_list):\n",
    "        point_clouds_object_list.append(open3d.PointCloud())\n",
    "        point_clouds_object_list[i].points = open3d.Vector3dVector(pcd)\n",
    "    open3d.draw_geometries(point_clouds_object_list)\n",
    "\n",
    "def affine_transform(pc, R, t):\n",
    "    return np.array([R.dot(a)+t for a in pc])\n",
    "    \n",
    "def background_removal(a_1):\n",
    "    valid_bool_1 = (a_1[:,2])<1\n",
    "    a_1 = a_1[valid_bool_1]\n",
    "    return a_1\n",
    "\n",
    "def informative_subsampling(normals, sample_size):\n",
    "    # convert normals to angular space\n",
    "    b = np.sqrt(normals[:,0]**2+normals[:,1]**2)\n",
    "    x = np.arctan2(normals[:,1], normals[:,0])\n",
    "    y = np.arctan2(normals[:,2], b)\n",
    "    \n",
    "    # devide normals over bins\n",
    "    bins = np.linspace(-np.pi, np.pi, sample_size) \n",
    "    x_index = np.digitize(x, bins, right=True)\n",
    "    y_index = np.digitize(y, bins, right=True)\n",
    "    index = x_index * sample_size + y_index\n",
    "\n",
    "    # uniformly sample from bins\n",
    "    unique_index, original_index = np.unique(index, return_index=True)\n",
    "    samples = np.random.choice(unique_index.shape[0], sample_size, replace=False)\n",
    "    sample_index = original_index[samples]\n",
    "    \n",
    "    # return only the found sample indices of the original pointcloud\n",
    "    return sample_index    \n",
    "\n",
    "def rms_error(p, q):\n",
    "    n = p.shape[0]\n",
    "    dist = [distance.euclidean(p[i,:], q[i,:]) for i in range(n)]\n",
    "    return np.sqrt(np.sum(np.power(dist, 2))/n)\n",
    "\n",
    "def remove_nan(points, normals):\n",
    "    keep = []\n",
    "    for nan in np.isnan(normals):\n",
    "        if (nan == True).any():\n",
    "            keep.append(False)\n",
    "        else:\n",
    "            keep.append(True)\n",
    "    points = points[keep]\n",
    "    normals = normals[keep]\n",
    "    return (points, normals)\n",
    "\n",
    "def rigid_motion(p,q):\n",
    "    \"\"\"\n",
    "    Least-Squares Rigid Motion Using Singular Value Decomposition. \n",
    "    (https://igl.ethz.ch/projects/ARAP/svd_rot.pdf) \n",
    "    \n",
    "    (note: so far only for the easy case, where all weights are = 1)\n",
    "    \n",
    "    p,q: shape [num_points, 3]\n",
    "    \n",
    "    \"\"\"\n",
    "    n,d = p.shape\n",
    "    \n",
    "    # compute centroids\n",
    "    p_cen = sum(p)/len(p)\n",
    "    q_cen = sum(q)/len(q)\n",
    "    \n",
    "    # compute centered vectors\n",
    "    X = np.array([i-p_cen for i in p])\n",
    "    Y = np.array([i-q_cen for i in q])\n",
    "    \n",
    "    # compute covariance matrix \n",
    "    W = np.eye(n)\n",
    "    S =  X.T.dot(W).dot(Y)\n",
    "    \n",
    "    # compute sigular value decomposition\n",
    "    U, _, V = np.linalg.svd(S)\n",
    "    \n",
    "    # compute optimal R and t\n",
    "    M = np.eye(d)\n",
    "    M[-1,-1] = np.linalg.det(V.T.dot(U.T))\n",
    "    R = V.T.dot(M).dot(U.T)\n",
    "    \n",
    "    t = q_cen - R.dot(p_cen)\n",
    "    \n",
    "    return R, t\n",
    "   \n",
    "def icp(a_1, a_2, n_1= None, n_2=None, convergence_treshold=1e-4, sampling_method=None, sample_size=1000, \n",
    "        stability_constant=1):\n",
    "    \"\"\"\n",
    "    a_1: positions of points in point cloud 1. shape : [num_points1, 3]\n",
    "    a_2: positions of points in point cloud 2. shape : [num_points2, 3]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the point clouds based on the depth,\n",
    "    # only keep the indices where the z of the point cloud is less than 1\n",
    "    a_1, a_2 = background_removal(a_1), background_removal(a_2) \n",
    "    a_2_c = a_2.copy()\n",
    "    \n",
    "    # Point sampling\n",
    "    if sampling_method == \"uniform\":\n",
    "        a_1 = a_1[np.random.randint(low=0, high=a_1.shape[0], size=sample_size)]\n",
    "        a_2 = a_2[np.random.randint(low=0, high=a_2.shape[0], size=sample_size)]\n",
    "        \n",
    "    if sampling_method == \"informative\":\n",
    "        print(\"?\")\n",
    "        #a_1 = informative_subsampling(source_file)\n",
    "        #a_2 = informative_subsampling(target_file)\n",
    "    \n",
    "    R_overall = np.eye(3)\n",
    "    t_overall = np.zeros(3)\n",
    "    \n",
    "    # Initialize RMS error for loop\n",
    "    rms_error_old = 10000\n",
    "    rms_error_new = rms_error_old-1\n",
    "    \n",
    "    while rms_error_old-rms_error_new > convergence_treshold:\n",
    "        \n",
    "        if sampling_method == \"random\":\n",
    "            a_1 = a_1[np.random.choice(a_1.shape[0], sample_size, replace=False), :]\n",
    "            a_2 = a_2[np.random.choice(a_2.shape[0], sample_size, replace=False), :]\n",
    "            \n",
    "        # (Step 1) Find closest points for each point in a_1 from a_2\n",
    "        tree = scipy.spatial.KDTree(a_2)\n",
    "        closest_dists, closest_idx = tree.query(a_1)\n",
    "        # Found this on stackoverflow: https://bit.ly/2P8IYiw\n",
    "        # Not sure if we can use this, but it is definetely much (!!) faster \n",
    "        # than manually comparing all the vectors.\n",
    "        # Usage also proposed on Wikipedia: https://bit.ly/2urg9nU\n",
    "        # For how-to-use see: https://bit.ly/2UbKNfn\n",
    "        closest_a_2 = a_2[closest_idx]\n",
    "    \n",
    "        # (Step 2) Refine R and t using Singular Value Decomposition\n",
    "        R, t = rigid_motion(a_1,closest_a_2)\n",
    "       \n",
    "        # update a_1\n",
    "        a_1 = affine_transform(a_1, R, t)\n",
    "        \n",
    "        # update rms error\n",
    "        rms_error_old = rms_error_new\n",
    "        rms_error_new = rms_error(a_1, closest_a_2)\n",
    "        \n",
    "        \n",
    "        # update overall R and t\n",
    "        R_overall = R.dot(R_overall)\n",
    "        t_overall = R.dot(t_overall) + t\n",
    "        \n",
    "    return R_overall, t_overall, a_1, rms_error_new\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pcds_from_filenames(filenames, is_normal = False):\n",
    "    \"\"\"\n",
    "    Read point clouds for given file names.\n",
    "    \"\"\"\n",
    "    if not is_normal:\n",
    "        return [open3d.read_point_cloud(f)for f in filenames]\n",
    "    else: \n",
    "        return [open3d.read_point_cloud(f, format = \"xyz\")for f in filenames]\n",
    "    \n",
    "## Exercise 3.1 (a) and (b)\n",
    "def merging_scenes(num_frames=99, sample_size=4000,frame_interval=1, perc_pcd_in=1,sampling_method='uniform', data_dir=\"./Data/data/\", max_images = 25):\n",
    "    \n",
    "    def sub_list(l, perc=0.05):\n",
    "        \"\"\" Sample random sublist.\"\"\"\n",
    "        le = a.shape[0] \n",
    "        rand_idx = np.random.choice(le,int(le*perc))\n",
    "        return l[rand_idx]\n",
    "\n",
    "    # Read all filenames for given directory\n",
    "    filenames = [data_dir+x for x in os.listdir(data_dir) \n",
    "                 if re.match(r\"00000000[0-9][0-9].pcd\",x)]\n",
    "    filenames.sort()\n",
    "    filenames = filenames[:num_frames]\n",
    "    \n",
    "    # Read all normal names\n",
    "    normals_filenames = [data_dir+x for x in os.listdir(data_dir) \n",
    "                        if re.match(r\"00000000[0-9][0-9]_normal.pcd\",x)]\n",
    "    normals_filenames.sort()\n",
    "    normals_filenames = normals_filenames[:num_frames]\n",
    "        \n",
    "    # Get relevant filenames (according to frame_interval)\n",
    "    filenames=filenames[0::frame_interval]\n",
    "    normals_filenames = normals_filenames[0::frame_interval]\n",
    "    \n",
    "    # Get point clouds for relevant filnames\n",
    "    pcds = read_pcds_from_filenames(filenames)\n",
    "    #pcd_points = [background_removal(remove_nan(np.array(p.points), n)[0]) for p, n in zip(pcds, normals)\n",
    "    # Get normals\n",
    "    normals = read_pcds_from_filenames(normals_filenames, True)\n",
    "    normals = [np.array(n.points) for n in normals]\n",
    "    pcd_points = [background_removal(remove_nan(np.array(p.points), n)[0]) for p, n in zip(pcds, normals)]\n",
    "    \n",
    "    # Tansform all frames back to zero-frame space \n",
    "    R_0 = np.eye(3)\n",
    "    t_0 = np.zeros(3)\n",
    "    transformed_points = []\n",
    "    transformations = []\n",
    "    total_points=np.array([])\n",
    "    \n",
    "    for i in range(len(pcd_points)-1,1,-1): # 99 -> 98 -> ... -> 0\n",
    "                \n",
    "        print(i)\n",
    "        \n",
    "        # perform icp from i to i-1\n",
    "        R, t, tp , _ = icp(pcd_points[i], pcd_points[i-1], sampling_method=sampling_method, sample_size=sample_size)\n",
    "        \n",
    "        # bring all previous point clouds in current space\n",
    "        transformed_points = [affine_transform(i,R,t) for i in transformed_points]\n",
    "       \n",
    "        # add ratio of current transformed point cloud \n",
    "        transformed_points.append(tp) \n",
    "        \n",
    "        \n",
    "    show_fitting_result(transformed_points)\n",
    "    \n",
    "    return transformed_points, transformations,pcd_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Example 3.1.:\n",
    "res, transformations, pcd_points = merging_scenes(frame_interval=1, num_frames=99, perc_pcd_in=1, sample_size=4000, sampling_method='uniform')\n",
    "show_fitting_result(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 3.2.:\n",
    "def merging_scenes_iteratively(features_for_total=300,num_frames=99, sample_size=4000,frame_interval=1, perc_pcd_in=1,sampling_method='uniform', data_dir=\"./Data/data/\", max_images = 25):\n",
    "    \n",
    "    def sub_list(l, a):\n",
    "        \"\"\" Sample random sublist.\"\"\"\n",
    "        le = l.shape[0] \n",
    "        rand_idx = np.random.choice(le,a)\n",
    "        return l[rand_idx]\n",
    "\n",
    "    def merge_to_new_frame(f1, f2):\n",
    "        e = int(f1.shape[0]/2)\n",
    "        i = sub_list(f1, e)\n",
    "        ii = sub_list(f2, e)\n",
    "        return np.concatenate((i,ii))\n",
    "\n",
    "    # Read all filenames for given directory\n",
    "    filenames = [data_dir+x for x in os.listdir(data_dir) \n",
    "                 if re.match(r\"00000000[0-9][0-9].pcd\",x)]\n",
    "    filenames.sort()\n",
    "    filenames = filenames[:num_frames]\n",
    "    \n",
    "    # Read all normal names\n",
    "    normals_filenames = [data_dir+x for x in os.listdir(data_dir) \n",
    "                        if re.match(r\"00000000[0-9][0-9]_normal.pcd\",x)]\n",
    "    normals_filenames.sort()\n",
    "    normals_filenames = normals_filenames[:num_frames]\n",
    "        \n",
    "    # Get relevant filenames (according to frame_interval)\n",
    "    filenames=filenames[0::frame_interval]\n",
    "    normals_filenames = normals_filenames[0::frame_interval]\n",
    "    \n",
    "    # Get point clouds for relevant filnames\n",
    "    pcds = read_pcds_from_filenames(filenames)\n",
    "    #pcd_points = [background_removal(remove_nan(np.array(p.points), n)[0]) for p, n in zip(pcds, normals)\n",
    "    # Get normals\n",
    "    normals = read_pcds_from_filenames(normals_filenames, True)\n",
    "    normals = [np.array(n.points) for n in normals]\n",
    "    pcd_points = [background_removal(remove_nan(np.array(p.points), n)[0]) for p, n in zip(pcds, normals)]\n",
    "    \n",
    "    # Tansform all frames back to zero-frame space \n",
    "    R_0 = np.eye(3)\n",
    "    t_0 = np.zeros(3)\n",
    "    transformed_points = []\n",
    "    transformations = []\n",
    "    total_points=np.array([])\n",
    "    \n",
    "    # Initialize\n",
    "    total = sub_list(pcd_points[0], 300)\n",
    "    \n",
    "    for i in range(1,len(pcd_points)-1): \n",
    "                \n",
    "        print(i+1) \n",
    "        \n",
    "        # perform icp from i to i-1\n",
    "        R, t, tp , _ = icp(total, sub_list(pcd_points[i],4000))\n",
    "        \n",
    "        #frame 1 (base) -Rt-> frame 2 (target)\n",
    "        total = np.concatenate((tp,sub_list(pcd_points[i],features_for_total)))\n",
    "\n",
    "    return total\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
